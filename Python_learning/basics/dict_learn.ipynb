{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc': 123} {'name': 'zhanghaiming'}\n",
      "{'name': 'zhanghaiming', 'height': 170}\n",
      "{'kitti': {'path': 123, 'num': 34}, 'nuscenes': {'path': 123, 'num': 34}} <class 'dict'> <class 'dict'>\n",
      "{'name': 'zhanghaiming', 'age': 18}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {'abc': 123}\n",
    "dict2 = {'name': 'zhanghaiming'}\n",
    "print(dict1, dict2)\n",
    "\n",
    "dict2['height'] = 170\n",
    "print(dict2)\n",
    "\n",
    "## Create dictionary of a dictionary\n",
    "dataset_config = {\n",
    "    ## KITTI\n",
    "    'kitti': {'path': 123, 'num': 34},\n",
    "    ## For nuScenes\n",
    "    'nuscenes': {'path': 123, 'num': 34}\n",
    "}\n",
    "\n",
    "print(dataset_config, type(dataset_config), type(dataset_config['kitti']))\n",
    "\n",
    "## Use dict() to create dictionary\n",
    "dict3 = dict(name='zhanghaiming', age=18)\n",
    "print(dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_loss': 3.4, 'gan_loss': 0.4}\n",
      "{'total_loss': 3.4, 'gan_loss': 0.4, 'l1_loss': 0.2, 'l2_loss': 1.3}\n"
     ]
    }
   ],
   "source": [
    "loss_names = ['total_loss', 'gan_loss']\n",
    "\n",
    "total_loss = 3.4\n",
    "gan_loss = 0.4\n",
    "\n",
    "loss_dict = dict(zip(loss_names, [total_loss, gan_loss]))\n",
    "print(loss_dict)\n",
    "\n",
    "l1_loss = 0.2\n",
    "l2_loss = 1.3\n",
    "loss_names = ['l1_loss', 'l2_loss']\n",
    "\n",
    "loss_dict = {**loss_dict, **dict(zip(loss_names, [l1_loss, l2_loss]))}\n",
    "print(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {'a':1, 'b':2}\n",
    "# dict1 = None\n",
    "# dict1 = False\n",
    "dict2 = {'c':3, 'd':4}\n",
    "\n",
    "dict3 = dict1 or dict2\n",
    "print(dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samples_per_gpu': 1, 'workers_per_gpu': 4}\n"
     ]
    }
   ],
   "source": [
    "test_dataloader_default_args = dict(samples_per_gpu=1, workers_per_gpu=2)\n",
    "test_dataloader = dict(workers_per_gpu=4)\n",
    "\n",
    "test_loader_cfg = {\n",
    "    **test_dataloader_default_args,\n",
    "    **test_dataloader\n",
    "}\n",
    "print(test_loader_cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create from a empty dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'0000': [1, 2, 3], '0001': [1, 2, 3]}}\n"
     ]
    }
   ],
   "source": [
    "data_dict = {'train':{}}\n",
    "\n",
    "sub_dict = {'0000': [1, 2, 3]}\n",
    "data_dict['train'].update(sub_dict)\n",
    "sub_dict = {'0001': [1, 2, 3]}\n",
    "data_dict['train'].update(sub_dict)\n",
    "\n",
    "# data_dict['train']['00000'] = 10\n",
    "\n",
    "print(data_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': 1, 'h': 8, 'c': 0, 'e': 12, 'f': 8}\n",
      "{'c': 0, 'e': 12, 'f': 8, 'g': 1, 'h': 8}\n",
      "{'c': 0, 'g': 1, 'h': 8, 'f': 8, 'e': 12}\n"
     ]
    }
   ],
   "source": [
    "input_dict = {'g':1, 'h':8, 'c':0, 'e': 12, 'f': 8}\n",
    "print(input_dict)\n",
    "\n",
    "# sort the dictionary by key\n",
    "sorted_dict = dict(sorted(input_dict.items(), key=lambda x: x[0]))\n",
    "print(sorted_dict)\n",
    "\n",
    "# sort the dictionary by value\n",
    "sorted_dict = dict(sorted(input_dict.items(), key=lambda x: x[1]))\n",
    "print(sorted_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update dictionary with another dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'li', 'height': 170, 'weight': 55}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {\"name\": \"zhang\", \"height\": 170}\n",
    "dict2 = {\"name\": \"li\", \"weight\": 55}\n",
    "\n",
    "dict1.update(dict2)\n",
    "print(dict1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove elements in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method1\n",
      "{'Ghana': 'Accra'} {'Ghana': 'Accra', 'China': 'Beijing'}\n",
      "None\n",
      "{'Ghana': 'Accra', 'China': 'Beijing'}\n"
     ]
    }
   ],
   "source": [
    "def method1(input_dict):\n",
    "    print(\"method1\")\n",
    "    res_dict = input_dict.copy()\n",
    "    res_dict.pop(\"China\")\n",
    "    return res_dict\n",
    "\n",
    "countries = {\"Ghana\": \"Accra\", \"China\": \"Beijing\"}\n",
    "\n",
    "res = method1(countries)\n",
    "print(res, countries)\n",
    "\n",
    "print(countries.pop(\"America\", None))\n",
    "print(countries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Member functions usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'height', 'weight']) ['name', 'height', 'weight']\n",
      "dict_values(['zhanghaiming', 170, 55]) <class 'dict_values'> ['zhanghaiming', 170, 55]\n",
      "55\n",
      "dictionary keys:  [3048, 3049, 3050, 3051, 'zhanghaiming']\n",
      "torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dict_a = {\"name\":\"zhanghaiming\", \"height\":170, \"weight\":55}\n",
    "### keys()和values()函数都返回迭代器对象,可以用list转为列表\n",
    "keys = dict_a.keys()\n",
    "print(keys, list(keys))\n",
    "\n",
    "values = dict_a.values()\n",
    "\n",
    "print(values, type(values), list(values))\n",
    "\n",
    "list_values = list(values)\n",
    "print(list_values[2])\n",
    "\n",
    "data_a = {98: 3048, 99: 3049, 100: 3050, 102: 3051, \"name\": \"zhanghaiming\"}\n",
    "data_a_keys = list(data_a.values()) # will be in original order\n",
    "print(\"dictionary keys: \", data_a_keys)\n",
    "\n",
    "import torch\n",
    "def stack_dict_values():\n",
    "    dict_arr = {'image_a': torch.randn(3, 3, 256, 256), 'image_b': torch.randn(5, 3, 256, 256)}\n",
    "    values = list(dict_arr.values())\n",
    "    value_arr = torch.concat(values, dim=0)\n",
    "    print(value_arr.shape)\n",
    "\n",
    "stack_dict_values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'zhanghaiming', 'height': 170, 'weight': 55} 3\n",
      "name is zhanghaiming\n",
      "weight is 55\n",
      "name\n",
      "height\n",
      "weight\n",
      "name\n",
      "height\n",
      "weight\n",
      "('name', 'zhanghaiming')\n",
      "('height', 170)\n",
      "('weight', 55)\n",
      "name zhanghaiming\n",
      "height 170\n",
      "weight 55\n",
      "------------Access keys not exist---------------\n",
      "dict_keys(['name', 'height', 'weight'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "dict_a = {\"name\":\"zhanghaiming\", \"height\":170, \"weight\":55}\n",
    "print(dict_a, len(dict_a))\n",
    "\n",
    "### 直接键访问\n",
    "print(f\"name is {dict_a['name']}\")\n",
    "print(f\"weight is {dict_a['weight']}\")\n",
    "\n",
    "### 遍历访问\n",
    "for key in dict_a: # 默认只得到键值\n",
    "    print(key)\n",
    "for key in dict_a.keys(): # 同上\n",
    "    print(key)\n",
    "\n",
    "for ent in dict_a.items(): # items返回键值对元组\n",
    "    print(ent)\n",
    "\n",
    "for (key, value) in dict_a.items(): # 这样就可以灵活取到想要的值了\n",
    "    print(key, value)\n",
    "\n",
    "### Access keys not exist\n",
    "print(\"------------Access keys not exist---------------\")\n",
    "print(dict_a.keys())\n",
    "print(dict_a.get(\"age\", 20)) # if use dict_a[\"age\"] would get error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n"
     ]
    }
   ],
   "source": [
    "loss_dict = {\"loss_rec\": [], }\n",
    "\n",
    "loss_dict[\"loss_rec\"].append(1)\n",
    "loss_dict[\"loss_rec\"].append(2)\n",
    "\n",
    "print(type(loss_dict[\"loss_rec\"]), len(loss_dict[\"loss_rec\"]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "from weakref import WeakValueDictionary\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defaultdict class Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.3, 1.5]\n",
      "{'total_loss': 1.2666666666666666}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "loss_dict = defaultdict(list)\n",
    "loss_dict[\"total_loss\"].append(1.0)\n",
    "loss_dict[\"total_loss\"].append(1.3)\n",
    "loss_dict[\"total_loss\"].append(1.5)\n",
    "\n",
    "print(loss_dict[\"total_loss\"])\n",
    "\n",
    "def calc_mean_loss(loss_dict):\n",
    "    loss_mean = dict()\n",
    "    for key, loss_list in loss_dict.items():\n",
    "        loss_mean[key] = sum(loss_list) / len(loss_list)\n",
    "    return loss_mean\n",
    "\n",
    "mean_value = calc_mean_loss(loss_dict)\n",
    "print(mean_value)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "loss_dict = defaultdict(AverageMeter)\n",
    "loss_dict[\"total_loss\"].update(1.0)\n",
    "loss_dict[\"total_loss\"].update(1.3)\n",
    "loss_dict[\"total_loss\"].update(1.5)\n",
    "\n",
    "print(loss_dict[\"total_loss\"].count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine defaultdict and python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name <class 'str'> id0001\n",
      "source_image <class 'list'> [1, 2]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "data = defaultdict(list)\n",
    "data['source_image'].append(1)\n",
    "data['source_image'].append(2)\n",
    "\n",
    "data_res = dict()\n",
    "data_res['name'] = 'id0001'\n",
    "data_res.update(data)\n",
    "\n",
    "for key, value in data_res.items():\n",
    "    print(key, type(value), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_loss': 3.4000000000000004, 'l1_loss': 3.066666666666667}\n"
     ]
    }
   ],
   "source": [
    "loss_dict_1 = {'total_loss': 3.4, \"l1_loss\": 4.4}\n",
    "loss_dict_2 = {'total_loss': 5.4, \"l1_loss\": 3.4}\n",
    "loss_dict_3 = {'total_loss': 1.4, \"l1_loss\": 1.4}\n",
    "\n",
    "loss_dict_list = [loss_dict_1, loss_dict_2, loss_dict_3]\n",
    "# loss_dict_list = []\n",
    "\n",
    "\n",
    "def calc_avg_loss(loss_dict_list):\n",
    "    assert len(loss_dict_list) != False, \"input list length is 0\"\n",
    "\n",
    "    avg_loss_dict = dict()\n",
    "\n",
    "    all_loss_keys = loss_dict_list[0].keys()\n",
    "\n",
    "    for key in all_loss_keys:\n",
    "        loss_list = [loss_dict[key] for loss_dict in loss_dict_list]\n",
    "        avg_loss_dict[key] = sum(loss_list) / len(loss_list)\n",
    "\n",
    "    return avg_loss_dict\n",
    "\n",
    "avg_loss_dict = calc_avg_loss(loss_dict_list)     \n",
    "print(avg_loss_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EasyDict usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_paste': 50, 'num_obj': 10} <class 'easydict.EasyDict'>\n",
      "50\n",
      "50 10\n",
      "100\n",
      "{'batch_size': 5, 'distributed': False}\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "config = EasyDict(num_paste=50)\n",
    "config.num_obj = 10 # Use dictionary like a class member\n",
    "print(config, type(config))\n",
    "print(config['num_paste'])\n",
    "\n",
    "\n",
    "def parse_dict_args(num_paste=1, num_obj=1):\n",
    "    print(num_paste, num_obj)\n",
    "\n",
    "parse_dict_args(**vars(config))\n",
    "\n",
    "## Get the key not exists in the dict\n",
    "print(config.get(\"name\", 100))\n",
    "\n",
    "opt = EasyDict()\n",
    "opt.type = \"data.vox_dataset::VoxDataset\"\n",
    "opt.path = \"./dataset/vox_lmdb_demo\"\n",
    "opt.resolution = 256\n",
    "opt.semantic_radius = 13\n",
    "opt.train = EasyDict()\n",
    "opt.val = EasyDict()\n",
    "opt.train.batch_size = 5\n",
    "opt.train.distributed = False\n",
    "opt.val.batch_size = 5\n",
    "opt.val.distributed = False\n",
    "print(opt['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scheduler': 'lr_scheduler', 'interval': 'epoch', 'frequency': 4000}\n",
      "False <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "config = EasyDict(interval=\"epoch\", frequency=4000)\n",
    "\n",
    "scheduler = {'scheduler': \"lr_scheduler\"}\n",
    "scheduler.update(config)\n",
    "print(scheduler)\n",
    "\n",
    "config.is_predict_patch = False\n",
    "data = getattr(config, \"is_predict_patch\", True)\n",
    "print(data, type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38-torch100-cu11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56d0891f27fe2db3830dc2a05bc70f05135b0c30ed7c190a150d1aca2da3af60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
