{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn how to use the einops library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import repeat, rearrange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use repeat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0445,  1.2679,  0.0591],\n",
      "        [ 0.8185, -0.0033, -1.2111]]) torch.Size([2, 3])\n",
      "tensor([[[-0.0445, -0.0445, -0.0445],\n",
      "         [ 1.2679,  1.2679,  1.2679],\n",
      "         [ 0.0591,  0.0591,  0.0591]],\n",
      "\n",
      "        [[ 0.8185,  0.8185,  0.8185],\n",
      "         [-0.0033, -0.0033, -0.0033],\n",
      "         [-1.2111, -1.2111, -1.2111]]]) torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 3)\n",
    "print(input, input.shape)\n",
    "\n",
    "output = repeat(input, 'h w -> h w c', c=3)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat a whole matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]]) torch.Size([2, 4, 4])\n",
      "tensor([[0, 1, 2, 3, 4]]) \n",
      " tensor([[[0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4]]]) torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import repeat\n",
    "\n",
    "intrinsics_pad = repeat(torch.eye(4), \"X Y -> L X Y\", L = 2).clone()\n",
    "print(intrinsics_pad, intrinsics_pad.shape)\n",
    "\n",
    "idx = torch.arange(5).reshape(1, 5)\n",
    "idx_repeat = repeat(idx, \"B RN -> B DimX RN\", DimX=3)\n",
    "print(idx, \"\\n\", idx_repeat, idx_repeat.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat specific axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5255, -2.6183,  0.1318],\n",
      "         [ 0.5025, -0.5046,  0.7587]]])\n",
      "tensor([[[-1.5255, -2.6183,  0.1318],\n",
      "         [ 0.5025, -0.5046,  0.7587]],\n",
      "\n",
      "        [[-1.5255, -2.6183,  0.1318],\n",
      "         [ 0.5025, -0.5046,  0.7587]]]) torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "pc = torch.randn(1, 2, 3)\n",
    "print(pc)\n",
    "\n",
    "### The below three methods are the same\n",
    "pc = repeat(pc, 'b n c -> (repeat b) n c', repeat=2).contiguous()\n",
    "# pc = repeat(pc, 'b n c -> (b 2) n c').contiguous()\n",
    "# pc = repeat(pc, 'b n c -> (2 b) n c').contiguous()\n",
    "print(pc, pc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.int64\n",
      "tensor([[9],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "class_label = 9\n",
    "\n",
    "batch_size = 2\n",
    "input = torch.tensor([class_label])\n",
    "\n",
    "c_indices = repeat(input, '1 -> b 1', b=batch_size)  # class token\n",
    "print(c_indices.shape, c_indices.dtype)\n",
    "print(c_indices)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rearange operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11]]) torch.Size([6, 2])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]]]) torch.Size([3, 2, 2])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]]]) torch.Size([2, 3, 2])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 2,  3],\n",
      "         [ 8,  9]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [10, 11]]]) torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "# input = torch.randn(4, 2)\n",
    "# print(input, input.shape)\n",
    "\n",
    "# output = rearrange(input, '(b n) c -> b n c', b=2)\n",
    "# print(output, output.shape)\n",
    "\n",
    "# output = input.reshape(2, 2, 2)\n",
    "# print(output, output.shape)\n",
    "\n",
    "input = torch.arange(12).reshape(6, 2)\n",
    "print(input, input.shape)\n",
    "\n",
    "output = rearrange(input, '(b n) c -> b n c', b=3)\n",
    "print(output, output.shape)\n",
    "\n",
    "output = rearrange(input, '(n b) c -> n b c', b=3)\n",
    "print(output, output.shape)\n",
    "\n",
    "output = rearrange(input, '(n b) c -> b n c', b=3)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expand dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2154, -1.8071, -1.5535]) torch.Size([3])\n",
      "tensor([[[[-0.2154]],\n",
      "\n",
      "         [[-1.8071]],\n",
      "\n",
      "         [[-1.5535]]]]) torch.Size([1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "input = torch.randn(3)\n",
    "print(input, input.shape)\n",
    "\n",
    "output = repeat(input, 'h -> () h () ()')\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim dim -> dim \n"
     ]
    },
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"dim dim -> dim \".\n Input tensor shape: (4, 5). Additional info: {}.\n Indexing expression contains duplicate dimension \"dim\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/data/zhanghm/miniconda3/envs/bevdet_py38/lib/python3.8/site-packages/einops/einops.py:411\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    410\u001b[0m hashable_axes_lengths \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(axes_lengths\u001b[39m.\u001b[39mitems()))\n\u001b[0;32m--> 411\u001b[0m recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[39m=\u001b[39;49mhashable_axes_lengths)\n\u001b[1;32m    412\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_recipe(recipe, tensor, reduction_type\u001b[39m=\u001b[39mreduction)\n",
      "File \u001b[0;32m/data/zhanghm/miniconda3/envs/bevdet_py38/lib/python3.8/site-packages/einops/einops.py:252\u001b[0m, in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_lengths)\u001b[0m\n\u001b[1;32m    251\u001b[0m left_str, rght_str \u001b[39m=\u001b[39m pattern\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m->\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m left \u001b[39m=\u001b[39m ParsedExpression(left_str)\n\u001b[1;32m    253\u001b[0m rght \u001b[39m=\u001b[39m ParsedExpression(rght_str)\n",
      "File \u001b[0;32m/data/zhanghm/miniconda3/envs/bevdet_py38/lib/python3.8/site-packages/einops/parsing.py:87\u001b[0m, in \u001b[0;36mParsedExpression.__init__\u001b[0;34m(self, expression, allow_underscore, allow_duplicates)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m current_identifier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     add_axis_name(current_identifier)\n\u001b[1;32m     88\u001b[0m current_identifier \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/zhanghm/miniconda3/envs/bevdet_py38/lib/python3.8/site-packages/einops/parsing.py:52\u001b[0m, in \u001b[0;36mParsedExpression.__init__.<locals>.add_axis_name\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (allow_underscore \u001b[39mand\u001b[39;00m x \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_duplicates:\n\u001b[0;32m---> 52\u001b[0m         \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39m'\u001b[39m\u001b[39mIndexing expression contains duplicate dimension \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(x))\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m _ellipsis:\n",
      "\u001b[0;31mEinopsError\u001b[0m: Indexing expression contains duplicate dimension \"dim\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m tensor2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[1;32m     27\u001b[0m tensor3 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m reshaped_tensor1 \u001b[39m=\u001b[39m reshape_first_two_dims(tensor1)\n\u001b[1;32m     30\u001b[0m reshaped_tensor2 \u001b[39m=\u001b[39m reshape_first_two_dims(tensor2)\n\u001b[1;32m     31\u001b[0m reshaped_tensor3 \u001b[39m=\u001b[39m reshape_first_two_dims(tensor3)\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mreshape_first_two_dims\u001b[0;34m(input_tensor)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(new_shape_pattern)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Reshape the tensor\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m reshaped_tensor \u001b[39m=\u001b[39m rearrange(input_tensor, new_shape_pattern)\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m reshaped_tensor\n",
      "File \u001b[0;32m/data/zhanghm/miniconda3/envs/bevdet_py38/lib/python3.8/site-packages/einops/einops.py:483\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRearrange can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be applied to an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    482\u001b[0m     tensor \u001b[39m=\u001b[39m get_backend(tensor[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstack_on_zeroth_dimension(tensor)\n\u001b[0;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(cast(Tensor, tensor), pattern, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrearrange\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maxes_lengths)\n",
      "File \u001b[0;32m/data/zhanghm/miniconda3/envs/bevdet_py38/lib/python3.8/site-packages/einops/einops.py:420\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    418\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Input is list. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    419\u001b[0m message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAdditional info: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 420\u001b[0m \u001b[39mraise\u001b[39;00m EinopsError(message \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"dim dim -> dim \".\n Input tensor shape: (4, 5). Additional info: {}.\n Indexing expression contains duplicate dimension \"dim\""
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "def reshape_first_two_dims(input_tensor):\n",
    "    # Get the number of dimensions in the input tensor\n",
    "    num_dims = input_tensor.ndim\n",
    "\n",
    "    if num_dims < 2:\n",
    "        # Handle cases where the tensor has fewer than 2 dimensions\n",
    "        raise ValueError(\"Input tensor must have at least 2 dimensions.\")\n",
    "\n",
    "    # Create the new shape pattern based on the number of dimensions\n",
    "    new_shape_pattern = f'{\" \".join([\"dim\"] * num_dims)} -> dim {\" \".join([\"dim\"] * (num_dims - 2))}'\n",
    "    print(new_shape_pattern)\n",
    "\n",
    "    # Reshape the tensor\n",
    "    reshaped_tensor = rearrange(input_tensor, new_shape_pattern)\n",
    "\n",
    "    return reshaped_tensor\n",
    "\n",
    "# Example usage:\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create tensors with varying dimensions\n",
    "tensor1 = np.random.rand(4, 5)\n",
    "tensor2 = np.random.rand(3, 4, 5)\n",
    "tensor3 = torch.randn(2, 3, 4, 5)\n",
    "\n",
    "reshaped_tensor1 = reshape_first_two_dims(tensor1)\n",
    "reshaped_tensor2 = reshape_first_two_dims(tensor2)\n",
    "reshaped_tensor3 = reshape_first_two_dims(tensor3)\n",
    "\n",
    "print(reshaped_tensor1.shape)\n",
    "print(reshaped_tensor2.shape)\n",
    "print(reshaped_tensor3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[ 0, 12],\n",
      "        [ 1, 13],\n",
      "        [ 2, 14],\n",
      "        [ 3, 15],\n",
      "        [ 4, 16],\n",
      "        [ 5, 17],\n",
      "        [ 6, 18],\n",
      "        [ 7, 19],\n",
      "        [ 8, 20],\n",
      "        [ 9, 21],\n",
      "        [10, 22],\n",
      "        [11, 23]]) torch.Size([12, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "data = torch.arange(24).reshape(2, 3, 4)\n",
    "print(data)\n",
    "\n",
    "# data = rearrange(data, 'b n c -> (b n) c')\n",
    "# data = rearrange(data, 'b n c -> b (n c)')\n",
    "data = rearrange(data, 'b n c -> (n c) b')\n",
    "# data = rearrange(data, 'b n c -> b (c n)')\n",
    "print(data, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256, 12])\n",
      "torch.Size([5, 256, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(5, 256, 12)\n",
    "print(input.shape)\n",
    "\n",
    "output = rearrange(input, 'b n (r c) -> b n r c', c=3)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "input = torch.randn(2, 5, 4, 3)\n",
    "\n",
    "input = rearrange(input, 'b g m c -> (b g) m c')\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2048, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "input = torch.randn(1, 262144, 3)\n",
    "\n",
    "output = rearrange(input, 'x (b n) c -> (x b) n c', b=128)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-torch100-cu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56d0891f27fe2db3830dc2a05bc70f05135b0c30ed7c190a150d1aca2da3af60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
