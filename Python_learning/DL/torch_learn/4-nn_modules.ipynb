{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### Get number of parameters in Module\n",
    "def get_num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> True\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "learn_data = nn.Parameter(torch.randn(10, 10))\n",
    "print(type(learn_data), learn_data.requires_grad)  # requires_grad is True by default\n",
    "print(type(learn_data.data), learn_data.shape)\n",
    "\n",
    "print(learn_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters in module is: 1480\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 3, 5, 5])\n",
      "torch.Size([10])\n",
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "buffer:  torch.Size([2, 3])\n",
      "emb torch.Size([10, 10]) True\n",
      "weight torch.Size([10, 10]) True\n",
      "conv.weight torch.Size([10, 3, 5, 5]) True\n",
      "conv.bias torch.Size([10]) True\n",
      "style.0.weight torch.Size([20, 1, 5, 5]) True\n",
      "style.0.bias torch.Size([20]) True\n",
      "Dummy(\n",
      "  (conv): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (style): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "odict_keys(['emb', 'weight', 'my_buffer', 'conv.weight', 'conv.bias', 'style.0.weight', 'style.0.bias'])\n"
     ]
    }
   ],
   "source": [
    "class Dummy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dummy, self).__init__()\n",
    "\n",
    "        emb = nn.Parameter(torch.randn(10, 10))\n",
    "        self.register_parameter('emb', emb)\n",
    "\n",
    "        buffer = torch.randn(2, 3)  # tensor\n",
    "        self.register_buffer('my_buffer', buffer)\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(10, 10))\n",
    "        self.conv = nn.Conv2d(3, 10, 5)\n",
    "        layers = [self.conv, self.conv]\n",
    "\n",
    "        self.style = nn.Sequential(nn.Conv2d(1, 20, 5), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.weight\n",
    "\n",
    "net = Dummy()\n",
    "\n",
    "print(\"The total number of parameters in module is:\", get_num_parameters(net))\n",
    "print('Total number of parameters: {:.3f}M'.format(get_num_parameters(net) / 1e6))\n",
    "\n",
    "params = net.parameters()\n",
    "for i in params:\n",
    "    print(i.shape)\n",
    "\n",
    "for buffer in net.buffers():\n",
    "    print(\"buffer: \", buffer.shape)\n",
    "\n",
    "for name, params in net.named_parameters():\n",
    "    print(name, params.shape, params.requires_grad)\n",
    "\n",
    "print(net)\n",
    "print(net.state_dict().keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 64, 64])\n",
      "The total number of parameters in module is:\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "norm = nn.BatchNorm2d(512)\n",
    "\n",
    "input = torch.randn(1, 512, 64, 64)\n",
    "output = norm(input)\n",
    "print(output.shape)\n",
    "\n",
    "print(\"The total number of parameters in module is:\")\n",
    "pytorch_total_params = get_num_parameters(norm)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| input_data.requires_grad: False, output.requires_grad: True\n",
      "ic| fc.weight.requires_grad: True, fc.bias.requires_grad: True\n",
      "ic| 'Use torch.no_grad'\n",
      "ic| fc.weight.requires_grad: True, fc.bias.requires_grad: True\n",
      "ic| input_data.requires_grad: False, output.requires_grad: False\n",
      "ic| 'Set network requires_grad False'\n",
      "ic| fc.weight.requires_grad: False, fc.bias.requires_grad: False\n",
      "ic| input_data.requires_grad: False, output.requires_grad: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from icecream import ic as print\n",
    "\n",
    "fc = nn.Linear(8, 128)\n",
    "\n",
    "input_data = torch.randn(64, 60, 8, requires_grad=True)\n",
    "input_data = torch.randn(64, 60, 8)\n",
    "# by default, the requires_grad of parameters is True\n",
    "output = fc(input_data)\n",
    "print(input_data.requires_grad, output.requires_grad) \n",
    "print(fc.weight.requires_grad, fc.bias.requires_grad)\n",
    "\n",
    "print(\"Use torch.no_grad\")\n",
    "with torch.no_grad():\n",
    "    output = fc(input_data)\n",
    "    print(fc.weight.requires_grad, fc.bias.requires_grad)\n",
    "    print(input_data.requires_grad, output.requires_grad)\n",
    "\n",
    "print(\"Set network requires_grad False\")\n",
    "input_data = torch.randn(64, 60, 8, requires_grad=True)\n",
    "input_data = torch.randn(64, 60, 8)\n",
    "fc.requires_grad_(False)\n",
    "print(fc.weight.requires_grad, fc.bias.requires_grad)\n",
    "output = fc(input_data)\n",
    "print(input_data.requires_grad, output.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8822, -0.3836, -0.4453,  0.4732],\n",
      "        [-0.2166,  1.7199,  0.8660, -1.1443],\n",
      "        [-1.3421,  0.9498, -0.0602, -0.1829]])\n",
      "tensor([[-0.8822, -0.3836, -0.4453,  0.4732],\n",
      "        [-0.2166,  1.7199,  0.8660, -1.1443],\n",
      "        [-1.3421,  0.9498, -0.0602, -0.1829]])\n"
     ]
    }
   ],
   "source": [
    "## To be as a placeholder in code, don't use any passed arguments\n",
    "m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n",
    "input = torch.randn(3, 4)\n",
    "print(input)\n",
    "\n",
    "output = m(input)  # Do nothing\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "1355\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "5529600.0 0\n",
      "5.530M 0.000B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## Common convolutional layers\n",
    "conv = nn.Conv2d(3, 5, 3, 2, 1)\n",
    "\n",
    "print(get_num_parameters(conv))\n",
    "\n",
    "conv = nn.Conv2d(30, 5, 3, 2, 1)\n",
    "print(get_num_parameters(conv))\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "flops, params = profile(conv, inputs=(torch.randn(1, 30, 128, 128),))\n",
    "print(flops, params) # 1819066368.0 11689512.0\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print(flops, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 3])\n",
      "torch.Size([5])\n",
      "Parameter containing:\n",
      "tensor([ 0.1460, -0.1008, -0.1073, -0.0464,  0.1341], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([5, 4, 3, 3])\n",
      "4 tensor([[ 0.6317,  1.4049, -0.9065],\n",
      "        [ 0.1092, -1.6640,  1.5237],\n",
      "        [-0.8378,  0.3228, -0.5971]])\n",
      "\n",
      "Apply convolution\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [5, 4, 3, 3], expected input[2, 3, 10, 10] to have 4 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/zhanghm/Research/Programming/programming-learning-examples/Python_learning/DL/torch_learn/4-nn_modules.ipynb Cell 16\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2253746f6e227d/home/zhanghm/Research/Programming/programming-learning-examples/Python_learning/DL/torch_learn/4-nn_modules.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mApply convolution\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2253746f6e227d/home/zhanghm/Research/Programming/programming-learning-examples/Python_learning/DL/torch_learn/4-nn_modules.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2253746f6e227d/home/zhanghm/Research/Programming/programming-learning-examples/Python_learning/DL/torch_learn/4-nn_modules.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m output \u001b[39m=\u001b[39m conv(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2253746f6e227d/home/zhanghm/Research/Programming/programming-learning-examples/Python_learning/DL/torch_learn/4-nn_modules.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-torch100-cu11/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-torch100-cu11/lib/python3.8/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-torch100-cu11/lib/python3.8/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    443\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [5, 4, 3, 3], expected input[2, 3, 10, 10] to have 4 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "### 反卷积 https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## Common convolutional layers\n",
    "conv = nn.Conv2d(4, 5, 3, 2, 1)\n",
    "print(conv.weight.shape)\n",
    "print(conv.bias.shape)\n",
    "print(conv.bias)\n",
    "conv.weight.data.normal_()\n",
    "conv.bias.data.zero_()\n",
    "print(conv.bias)\n",
    "\n",
    "weight = getattr(conv, \"weight\")\n",
    "print(weight.shape)\n",
    "print(weight.data.size(1), weight.data[0][0])\n",
    "\n",
    "## Apply convolution\n",
    "print(\"\\nApply convolution\")\n",
    "input = torch.randn(2, 3, 10, 10)\n",
    "output = conv(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 768, 8, 14, 14])\n",
      "torch.Size([5, 768, 1568])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv_module = nn.Conv3d(in_channels=3, out_channels=768, \n",
    "                        kernel_size = (1, 16, 16), \n",
    "                        stride=(1, 16, 16))\n",
    "\n",
    "input = torch.randn(5, 3, 8, 224, 224)\n",
    "output = conv_module(input)\n",
    "print(output.shape)\n",
    "\n",
    "output = output.flatten(2)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| output.shape: torch.Size([2, 64, 1, 1])\n",
      "ic| num_param_conv1: 65600\n",
      "ic| output.shape: torch.Size([2, 64])\n",
      "ic| num_param_fc: 65600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65600"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from icecream import ic as print\n",
    "\n",
    "conv1 = nn.Conv2d(1024, 64, 1, stride=1, padding=0)\n",
    "\n",
    "input = torch.rand(2, 1024, 1, 1)\n",
    "\n",
    "output = conv1(input)\n",
    "print(output.shape)\n",
    "\n",
    "num_param_conv1 = get_num_parameters(conv1)\n",
    "print(num_param_conv1)\n",
    "\n",
    "fc = nn.Linear(1024, 64)\n",
    "output = fc(input.squeeze())\n",
    "print(output.shape)\n",
    "\n",
    "num_param_fc = get_num_parameters(fc)\n",
    "print(num_param_fc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 200, 200, 16])\n",
      "torch.Size([1, 8, 200, 200, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class_emb = nn.Embedding(16, 8)\n",
    "\n",
    "input = torch.randint(0, 16, (1, 8, 200, 200, 16))\n",
    "print(input.shape)\n",
    "# print(input)\n",
    "\n",
    "output = class_emb(input)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n",
      "torch.Size([5, 768])\n",
      "torch.Size([4, 10])\n",
      "torch.Size([4, 10, 768]) tensor(-2.6352, grad_fn=<MinBackward1>) tensor(2.5464, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "image_emb = nn.Embedding(1024, 256)\n",
    "params = get_num_parameters(image_emb)\n",
    "print(params)\n",
    "\n",
    "image_emb = nn.Embedding(5, 768)\n",
    "print(image_emb.weight.shape)\n",
    "\n",
    "# input = torch.LongTensor(4,)\n",
    "tok = torch.tensor([0], dtype=torch.long)\n",
    "input = tok.repeat(4, 10)\n",
    "print(input.shape)\n",
    "output = image_emb(input)\n",
    "print(output.shape, output.min(), output.max())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Convolution output tensor size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 96, 1, 109, 109])\n",
      "MaxPool3D shape:  torch.Size([20, 96, 1, 54, 54])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0.5023,  0.5023,  0.4128,  ...,  0.4558,  0.3107,  0.4717],\n",
       "           [ 0.5023,  0.5793,  0.3253,  ...,  0.4558,  0.3143,  0.4888],\n",
       "           [ 0.3287,  0.2374,  0.5073,  ...,  0.3702,  0.3143,  0.4888],\n",
       "           ...,\n",
       "           [ 0.3105,  0.3293,  0.4435,  ...,  0.2628,  0.4014,  0.4014],\n",
       "           [ 0.2706,  0.3995,  0.4435,  ...,  0.3990,  0.2933,  0.1918],\n",
       "           [ 0.4065,  0.3995,  0.3467,  ...,  0.2890,  0.1323,  0.1790]]],\n",
       "\n",
       "\n",
       "         [[[ 0.2466,  0.2063,  0.1703,  ...,  0.0763, -0.0086, -0.0442],\n",
       "           [-0.0941,  0.2566,  0.1294,  ...,  0.0763, -0.1647, -0.1810],\n",
       "           [ 0.0423,  0.0423,  0.0011,  ...,  0.1201,  0.1201,  0.0577],\n",
       "           ...,\n",
       "           [ 0.0726,  0.0833,  0.2228,  ..., -0.0621,  0.2760,  0.0202],\n",
       "           [ 0.0406,  0.0515,  0.2228,  ..., -0.0695,  0.2760, -0.0345],\n",
       "           [ 0.0502,  0.0502, -0.0586,  ...,  0.0078, -0.0091,  0.1370]]],\n",
       "\n",
       "\n",
       "         [[[-0.0618, -0.0442, -0.0197,  ...,  0.1594, -0.0027,  0.0232],\n",
       "           [-0.0618, -0.0442,  0.0426,  ...,  0.0676, -0.0086, -0.0470],\n",
       "           [ 0.1308,  0.0585,  0.0426,  ...,  0.0143, -0.0086, -0.0090],\n",
       "           ...,\n",
       "           [ 0.0468, -0.0233, -0.0814,  ...,  0.0175,  0.0175, -0.0494],\n",
       "           [ 0.1129,  0.1129, -0.0887,  ...,  0.0175,  0.0175, -0.1180],\n",
       "           [ 0.0010, -0.0017,  0.2571,  ..., -0.0361,  0.0927, -0.0505]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 0.2509,  0.4387,  0.2295,  ...,  0.2031,  0.2946,  0.1963],\n",
       "           [ 0.2509,  0.5736,  0.5736,  ...,  0.3481,  0.1822,  0.2137],\n",
       "           [ 0.1537,  0.5736,  0.5736,  ...,  0.2343,  0.2053,  0.2312],\n",
       "           ...,\n",
       "           [ 0.2718,  0.2718,  0.2803,  ...,  0.3232,  0.2667,  0.2667],\n",
       "           [ 0.2624,  0.2938,  0.2938,  ...,  0.2976,  0.2667,  0.4562],\n",
       "           [ 0.2053,  0.1088,  0.2804,  ...,  0.2976,  0.2668,  0.4562]]],\n",
       "\n",
       "\n",
       "         [[[ 0.3457,  0.2919,  0.4179,  ...,  0.2497,  0.2360,  0.2414],\n",
       "           [ 0.3457,  0.2466,  0.4179,  ...,  0.4101,  0.1921,  0.2414],\n",
       "           [ 0.3052,  0.3779,  0.3300,  ...,  0.4101,  0.2782,  0.3212],\n",
       "           ...,\n",
       "           [ 0.3240,  0.3240,  0.2698,  ...,  0.5477,  0.5477,  0.1364],\n",
       "           [ 0.2836,  0.6287,  0.2918,  ...,  0.3551,  0.2601,  0.2802],\n",
       "           [ 0.1494,  0.3166,  0.3488,  ...,  0.2898,  0.2244,  0.1911]]],\n",
       "\n",
       "\n",
       "         [[[-0.1119, -0.1110, -0.0637,  ..., -0.0739,  0.0289,  0.0289],\n",
       "           [-0.0208, -0.0541, -0.0541,  ..., -0.1447, -0.0010, -0.0010],\n",
       "           [-0.0296, -0.2023, -0.0870,  ..., -0.2169, -0.1118, -0.0976],\n",
       "           ...,\n",
       "           [-0.1021, -0.0482, -0.0781,  ..., -0.1445,  0.0642,  0.0642],\n",
       "           [ 0.0928,  0.0928, -0.1996,  ..., -0.1243, -0.1445, -0.1456],\n",
       "           [-0.0655, -0.1001, -0.0298,  ..., -0.0312, -0.0289, -0.0828]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.2956,  0.2521,  0.3492,  ...,  0.4124,  0.2054,  0.2054],\n",
       "           [ 0.3747,  0.3747,  0.2708,  ...,  0.4124,  0.3118,  0.5770],\n",
       "           [ 0.3747,  0.3747,  0.3279,  ...,  0.3056,  0.2925,  0.5770],\n",
       "           ...,\n",
       "           [ 0.4550,  0.3774,  0.2954,  ...,  0.3297,  0.2861,  0.3759],\n",
       "           [ 0.4289,  0.3935,  0.2526,  ...,  0.3736,  0.3736,  0.3759],\n",
       "           [ 0.4289,  0.3935,  0.2448,  ...,  0.4771,  0.4771,  0.3066]]],\n",
       "\n",
       "\n",
       "         [[[ 0.0192,  0.0758,  0.0872,  ...,  0.0743,  0.1628,  0.0030],\n",
       "           [ 0.0192,  0.0141,  0.0872,  ..., -0.0422, -0.0452,  0.1567],\n",
       "           [ 0.0032,  0.0110,  0.0707,  ...,  0.0799,  0.0282, -0.0009],\n",
       "           ...,\n",
       "           [ 0.1294,  0.1294,  0.0923,  ..., -0.1033, -0.1164,  0.1744],\n",
       "           [ 0.1294,  0.1294,  0.0923,  ...,  0.1093,  0.1093,  0.0206],\n",
       "           [ 0.1304,  0.0416, -0.0220,  ...,  0.0480,  0.0480,  0.0206]]],\n",
       "\n",
       "\n",
       "         [[[-0.1246,  0.0306, -0.1703,  ...,  0.0753,  0.0639,  0.0286],\n",
       "           [-0.0026, -0.0026, -0.0791,  ..., -0.0407,  0.0271, -0.0579],\n",
       "           [-0.0668, -0.0136, -0.0066,  ...,  0.1175, -0.0751, -0.1198],\n",
       "           ...,\n",
       "           [ 0.0178, -0.0081, -0.0081,  ...,  0.0450,  0.0450,  0.0152],\n",
       "           [ 0.0069, -0.0523,  0.0754,  ...,  0.0785,  0.0785, -0.0956],\n",
       "           [-0.0204,  0.0210,  0.0754,  ...,  0.1251,  0.0785,  0.0942]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 0.3007,  0.2570,  0.2571,  ...,  0.3422,  0.3081,  0.3081],\n",
       "           [ 0.3742,  0.2570,  0.3281,  ...,  0.2359,  0.2189,  0.3226],\n",
       "           [ 0.3603,  0.3603,  0.1609,  ...,  0.2491,  0.2491,  0.3226],\n",
       "           ...,\n",
       "           [ 0.2085,  0.1800,  0.2392,  ...,  0.2622,  0.2167,  0.2352],\n",
       "           [ 0.2085,  0.1223,  0.2600,  ...,  0.2791,  0.1863,  0.2699],\n",
       "           [ 0.4290,  0.1496,  0.2600,  ...,  0.2481,  0.2673,  0.1111]]],\n",
       "\n",
       "\n",
       "         [[[ 0.2587,  0.3661,  0.4704,  ...,  0.3491,  0.2483,  0.3651],\n",
       "           [ 0.4884,  0.2590,  0.3338,  ...,  0.2271,  0.2271,  0.2363],\n",
       "           [ 0.5324,  0.5324,  0.3058,  ...,  0.2271,  0.2572,  0.2305],\n",
       "           ...,\n",
       "           [ 0.4657,  0.3103,  0.2466,  ...,  0.2244,  0.4276,  0.4276],\n",
       "           [ 0.2842,  0.4036,  0.4534,  ...,  0.3096,  0.4807,  0.4807],\n",
       "           [ 0.2765,  0.3248,  0.5535,  ...,  0.4014,  0.3096,  0.4370]]],\n",
       "\n",
       "\n",
       "         [[[ 0.0340,  0.0340, -0.1443,  ..., -0.0049,  0.0433,  0.0433],\n",
       "           [-0.0086, -0.1211, -0.1443,  ...,  0.0269, -0.1393, -0.0741],\n",
       "           [-0.0136, -0.0030, -0.0207,  ...,  0.0269, -0.0622,  0.0592],\n",
       "           ...,\n",
       "           [-0.0362, -0.0394, -0.0394,  ..., -0.0765, -0.0802, -0.0802],\n",
       "           [-0.0362, -0.0746, -0.0989,  ...,  0.0694, -0.1975, -0.2210],\n",
       "           [-0.0746, -0.0393, -0.0393,  ...,  0.1564,  0.1564, -0.1049]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.2299,  0.4190,  0.3948,  ...,  0.3459,  0.2851,  0.2716],\n",
       "           [ 0.4120,  0.2556,  0.2556,  ...,  0.3954,  0.3177,  0.2716],\n",
       "           [ 0.3816,  0.4560,  0.2565,  ...,  0.3954,  0.3095,  0.4985],\n",
       "           ...,\n",
       "           [ 0.3111,  0.2704,  0.5601,  ...,  0.2508,  0.2636,  0.2636],\n",
       "           [ 0.4437,  0.5831,  0.5601,  ...,  0.5239,  0.5239,  0.3833],\n",
       "           [ 0.3209,  0.5831,  0.3976,  ...,  0.5239,  0.5239,  0.3273]]],\n",
       "\n",
       "\n",
       "         [[[ 0.0428,  0.0263,  0.0345,  ...,  0.1070,  0.0269,  0.3842],\n",
       "           [-0.0087, -0.0191,  0.0892,  ..., -0.0094,  0.0618,  0.3842],\n",
       "           [ 0.2476,  0.2476, -0.0238,  ...,  0.1614,  0.0823,  0.0866],\n",
       "           ...,\n",
       "           [ 0.0702,  0.1788,  0.1788,  ...,  0.1535,  0.1535,  0.0500],\n",
       "           [ 0.1694,  0.1267,  0.1762,  ...,  0.1535,  0.1535,  0.0885],\n",
       "           [ 0.1764,  0.1267, -0.1052,  ...,  0.0485,  0.0485,  0.0885]]],\n",
       "\n",
       "\n",
       "         [[[ 0.0870,  0.0870,  0.0424,  ..., -0.0033, -0.0477, -0.0498],\n",
       "           [-0.0371, -0.1039, -0.0232,  ..., -0.0266,  0.0201,  0.0363],\n",
       "           [ 0.0818, -0.0634,  0.0427,  ..., -0.0266,  0.0201,  0.0363],\n",
       "           ...,\n",
       "           [ 0.0066, -0.2129,  0.0143,  ...,  0.1125, -0.0253, -0.0955],\n",
       "           [ 0.1566, -0.0205,  0.0143,  ...,  0.0208,  0.0208, -0.0085],\n",
       "           [-0.0081, -0.0144,  0.0278,  ...,  0.2102,  0.0208, -0.0085]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 0.4003,  0.3631,  0.3631,  ...,  0.1002,  0.0750,  0.2281],\n",
       "           [ 0.0713,  0.3187,  0.2460,  ...,  0.1618,  0.0508,  0.2281],\n",
       "           [ 0.1670,  0.1826,  0.2460,  ...,  0.1119,  0.2073,  0.2708],\n",
       "           ...,\n",
       "           [ 0.2174,  0.2174,  0.2651,  ...,  0.3035,  0.1258,  0.2291],\n",
       "           [ 0.3197,  0.3197,  0.3146,  ...,  0.3962,  0.1850,  0.1551],\n",
       "           [ 0.3197,  0.3197,  0.5086,  ...,  0.1850,  0.2335,  0.3022]]],\n",
       "\n",
       "\n",
       "         [[[ 0.4414,  0.2344,  0.3250,  ...,  0.2695,  0.3137,  0.2869],\n",
       "           [ 0.4414,  0.3576,  0.3576,  ...,  0.3041,  0.3137,  0.2999],\n",
       "           [ 0.4319,  0.4319,  0.3576,  ...,  0.4161,  0.3136,  0.2629],\n",
       "           ...,\n",
       "           [ 0.2604,  0.4199,  0.4447,  ...,  0.3416,  0.2966,  0.3219],\n",
       "           [ 0.5108,  0.2496,  0.5464,  ...,  0.2475,  0.2651,  0.4104],\n",
       "           [ 0.5108,  0.3127,  0.5464,  ...,  0.5731,  0.1261,  0.1236]]],\n",
       "\n",
       "\n",
       "         [[[-0.0855, -0.0855, -0.0951,  ..., -0.1153, -0.1153, -0.1869],\n",
       "           [-0.0855, -0.0855, -0.1420,  ...,  0.1124,  0.1124, -0.1156],\n",
       "           [-0.0183, -0.0183, -0.2367,  ...,  0.1124,  0.1124, -0.0613],\n",
       "           ...,\n",
       "           [-0.0426, -0.0718, -0.0200,  ...,  0.1302,  0.0487, -0.0629],\n",
       "           [-0.0432, -0.1403, -0.0200,  ...,  0.1302, -0.1926, -0.0629],\n",
       "           [-0.1500, -0.0481, -0.0891,  ...,  0.0237, -0.1062, -0.0217]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.6227,  0.2850,  0.2607,  ...,  0.2330,  0.3754,  0.3794],\n",
       "           [ 0.6227,  0.3642,  0.3153,  ...,  0.3771,  0.4281,  0.4926],\n",
       "           [ 0.3642,  0.3642,  0.3153,  ...,  0.4218,  0.2403,  0.4926],\n",
       "           ...,\n",
       "           [ 0.4798,  0.4798,  0.2777,  ...,  0.3128,  0.2032,  0.2608],\n",
       "           [ 0.4798,  0.4798,  0.4785,  ...,  0.2827,  0.1642,  0.2990],\n",
       "           [ 0.2192,  0.4262,  0.2636,  ...,  0.4402,  0.4741,  0.2990]]],\n",
       "\n",
       "\n",
       "         [[[ 0.2228,  0.1640,  0.1640,  ...,  0.3380,  0.0599,  0.0607],\n",
       "           [ 0.0729, -0.0867, -0.0402,  ...,  0.3380,  0.1894, -0.0582],\n",
       "           [ 0.0220, -0.0763, -0.0402,  ..., -0.0574, -0.1167,  0.0170],\n",
       "           ...,\n",
       "           [ 0.0060, -0.0590, -0.0287,  ...,  0.0890,  0.0174,  0.0927],\n",
       "           [ 0.0060,  0.0665,  0.1197,  ..., -0.0013, -0.0027,  0.0926],\n",
       "           [ 0.0836,  0.0665,  0.0665,  ...,  0.0115, -0.1594,  0.0926]]],\n",
       "\n",
       "\n",
       "         [[[-0.1020, -0.1041,  0.0209,  ...,  0.0804, -0.0314, -0.1225],\n",
       "           [-0.0321, -0.0321, -0.1543,  ...,  0.0804,  0.0017, -0.0559],\n",
       "           [-0.1471,  0.0562,  0.0876,  ..., -0.0221, -0.0221,  0.0155],\n",
       "           ...,\n",
       "           [-0.0651, -0.1933,  0.0751,  ..., -0.0738, -0.0117, -0.0372],\n",
       "           [-0.0949, -0.1533, -0.0401,  ..., -0.0489,  0.1647,  0.0131],\n",
       "           [-0.0364, -0.0401, -0.0401,  ..., -0.1052,  0.0373, -0.0759]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 0.2640,  0.2631,  0.2858,  ...,  0.4154,  0.2508,  0.2508],\n",
       "           [ 0.2863,  0.2641,  0.2427,  ...,  0.1720,  0.2508,  0.2508],\n",
       "           [ 0.2524,  0.2641,  0.2969,  ...,  0.2287,  0.2171,  0.2825],\n",
       "           ...,\n",
       "           [ 0.1659,  0.2067,  0.3871,  ...,  0.4218,  0.4218,  0.4200],\n",
       "           [ 0.1659,  0.2792,  0.3871,  ...,  0.2089,  0.2244,  0.2244],\n",
       "           [ 0.2344,  0.2792,  0.2862,  ...,  0.2249,  0.2244,  0.2457]]],\n",
       "\n",
       "\n",
       "         [[[ 0.2950,  0.3120,  0.3004,  ...,  0.1980,  0.4475,  0.4893],\n",
       "           [ 0.2950,  0.2642,  0.5138,  ...,  0.2202,  0.4475,  0.4475],\n",
       "           [ 0.4297,  0.4022,  0.5138,  ...,  0.4784,  0.3525,  0.5241],\n",
       "           ...,\n",
       "           [ 0.2095,  0.2812,  0.2032,  ...,  0.3705,  0.3205,  0.2798],\n",
       "           [ 0.3693,  0.3693,  0.3171,  ...,  0.3900,  0.2585,  0.2152],\n",
       "           [ 0.2869,  0.2869,  0.4742,  ...,  0.3386,  0.2750,  0.3007]]],\n",
       "\n",
       "\n",
       "         [[[-0.1125, -0.1402, -0.0027,  ..., -0.0148, -0.1283, -0.1244],\n",
       "           [-0.1125, -0.1566, -0.0383,  ..., -0.0148, -0.1583, -0.1583],\n",
       "           [-0.0921, -0.0921, -0.1460,  ..., -0.0279, -0.0449, -0.0305],\n",
       "           ...,\n",
       "           [-0.0661, -0.0661, -0.0494,  ..., -0.0064, -0.0881, -0.0881],\n",
       "           [-0.0238, -0.0238, -0.1450,  ...,  0.1752, -0.1930, -0.1090],\n",
       "           [-0.1287, -0.1287, -0.0357,  ...,  0.0169, -0.1742, -0.1854]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.2679,  0.3420,  0.1833,  ...,  0.5086,  0.2886,  0.3912],\n",
       "           [ 0.2814,  0.3989,  0.1833,  ...,  0.5086,  0.1877,  0.3956],\n",
       "           [ 0.4409,  0.4397,  0.3534,  ...,  0.3664,  0.3664,  0.3956],\n",
       "           ...,\n",
       "           [ 0.1821,  0.3873,  0.3873,  ...,  0.3781,  0.3076,  0.3512],\n",
       "           [ 0.3934,  0.3430,  0.2008,  ...,  0.3781,  0.2831,  0.3511],\n",
       "           [ 0.3934,  0.3430,  0.3525,  ...,  0.2861,  0.3422,  0.3818]]],\n",
       "\n",
       "\n",
       "         [[[ 0.1109, -0.0139,  0.0291,  ...,  0.2390,  0.2390,  0.0571],\n",
       "           [ 0.2073,  0.0146, -0.0681,  ...,  0.0616,  0.0616, -0.0516],\n",
       "           [ 0.0119,  0.0119, -0.0667,  ...,  0.1848,  0.2164,  0.2244],\n",
       "           ...,\n",
       "           [ 0.0265,  0.0908,  0.1468,  ...,  0.1845,  0.0729,  0.1439],\n",
       "           [ 0.2564,  0.1083, -0.0022,  ..., -0.0103, -0.0103, -0.0503],\n",
       "           [ 0.1064, -0.0308, -0.0270,  ...,  0.1184,  0.0093, -0.0503]]],\n",
       "\n",
       "\n",
       "         [[[ 0.1427,  0.1613, -0.0047,  ..., -0.2437,  0.0772,  0.0772],\n",
       "           [ 0.1427,  0.0523, -0.0047,  ..., -0.1787,  0.1979,  0.0210],\n",
       "           [ 0.0422, -0.0444,  0.1775,  ...,  0.1420,  0.1979,  0.0210],\n",
       "           ...,\n",
       "           [ 0.1443,  0.1443,  0.1208,  ...,  0.2169, -0.0063, -0.0063],\n",
       "           [-0.0035,  0.1340,  0.1340,  ...,  0.1375, -0.0063, -0.0063],\n",
       "           [ 0.0471,  0.1340,  0.1340,  ...,  0.0109,  0.0044, -0.0749]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 0.4343,  0.2091,  0.1234,  ...,  0.1602,  0.2052,  0.3429],\n",
       "           [ 0.4343,  0.2189,  0.2613,  ...,  0.1645,  0.3663,  0.3663],\n",
       "           [ 0.2225,  0.2189,  0.2648,  ...,  0.1645,  0.1834,  0.3179],\n",
       "           ...,\n",
       "           [ 0.1885,  0.1885,  0.2043,  ...,  0.2756,  0.1722,  0.2313],\n",
       "           [ 0.2455,  0.3903,  0.3903,  ...,  0.4096,  0.4096,  0.4582],\n",
       "           [ 0.4545,  0.4545,  0.3903,  ...,  0.4096,  0.4096,  0.4582]]],\n",
       "\n",
       "\n",
       "         [[[ 0.3809,  0.3809,  0.3589,  ...,  0.4116,  0.4116,  0.3270],\n",
       "           [ 0.3809,  0.3809,  0.3070,  ...,  0.3090,  0.3738,  0.1759],\n",
       "           [ 0.4360,  0.3408,  0.5298,  ...,  0.3416,  0.3860,  0.2911],\n",
       "           ...,\n",
       "           [ 0.4858,  0.4301,  0.2912,  ...,  0.3006,  0.3109,  0.4676],\n",
       "           [ 0.3554,  0.2055,  0.3492,  ...,  0.3012,  0.3109,  0.4676],\n",
       "           [ 0.3354,  0.3354,  0.3492,  ...,  0.3870,  0.3325,  0.3019]]],\n",
       "\n",
       "\n",
       "         [[[-0.1092,  0.0451,  0.0451,  ...,  0.0232,  0.0250, -0.0537],\n",
       "           [-0.2224,  0.0451,  0.0451,  ..., -0.1316,  0.0008,  0.0008],\n",
       "           [-0.1822, -0.0416, -0.0416,  ..., -0.1316,  0.0008,  0.0008],\n",
       "           ...,\n",
       "           [ 0.0191,  0.0191,  0.0736,  ...,  0.2348,  0.1277, -0.0411],\n",
       "           [ 0.0191,  0.0191, -0.0602,  ..., -0.2137, -0.0411, -0.0411],\n",
       "           [-0.1499, -0.0506, -0.0404,  ..., -0.2137,  0.1962,  0.1962]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.2975,  0.3022,  0.4380,  ...,  0.5485,  0.4105,  0.4105],\n",
       "           [ 0.3683,  0.2572,  0.3302,  ...,  0.5485,  0.2265,  0.3856],\n",
       "           [ 0.4333,  0.4036,  0.4178,  ...,  0.2844,  0.3173,  0.3173],\n",
       "           ...,\n",
       "           [ 0.3458,  0.2512,  0.3621,  ...,  0.2179,  0.2652,  0.4108],\n",
       "           [ 0.4289,  0.3407,  0.3407,  ...,  0.2684,  0.2865,  0.4108],\n",
       "           [ 0.4811,  0.5101,  0.3407,  ...,  0.2183,  0.2594,  0.3774]]],\n",
       "\n",
       "\n",
       "         [[[ 0.1445,  0.1445,  0.0399,  ..., -0.0763,  0.0818,  0.0818],\n",
       "           [ 0.0417, -0.0094, -0.0172,  ..., -0.0154,  0.1163, -0.0340],\n",
       "           [ 0.1462,  0.1515,  0.2331,  ..., -0.0682,  0.1163, -0.0106],\n",
       "           ...,\n",
       "           [-0.0071,  0.1501,  0.2036,  ...,  0.1523,  0.0473,  0.0474],\n",
       "           [-0.0317,  0.1501,  0.1501,  ..., -0.0123, -0.0414,  0.0474],\n",
       "           [ 0.0273,  0.0305,  0.1244,  ...,  0.1107,  0.1934,  0.1934]]],\n",
       "\n",
       "\n",
       "         [[[ 0.0836,  0.0836,  0.0530,  ..., -0.0008, -0.0796, -0.0796],\n",
       "           [ 0.0836,  0.0836,  0.0530,  ...,  0.1359, -0.2226, -0.0358],\n",
       "           [ 0.2307,  0.2054,  0.0359,  ...,  0.1359, -0.1005, -0.0358],\n",
       "           ...,\n",
       "           [ 0.1904, -0.0081, -0.0081,  ..., -0.0925,  0.0542,  0.0542],\n",
       "           [-0.1028, -0.0258, -0.0286,  ...,  0.1024,  0.1024,  0.0734],\n",
       "           [-0.0529, -0.0725, -0.0617,  ..., -0.2080, -0.1094, -0.0529]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 0.2206,  0.2300,  0.3239,  ...,  0.2416,  0.4066,  0.1943],\n",
       "           [ 0.1701,  0.2300,  0.3239,  ...,  0.2155,  0.2823,  0.2216],\n",
       "           [ 0.1683,  0.1730,  0.1045,  ...,  0.4853,  0.4853,  0.1833],\n",
       "           ...,\n",
       "           [ 0.1843,  0.3018,  0.3018,  ...,  0.5779,  0.3698,  0.2298],\n",
       "           [ 0.3834,  0.3834,  0.1603,  ...,  0.5779,  0.4078,  0.2298],\n",
       "           [ 0.3834,  0.3834,  0.1603,  ...,  0.2627,  0.4078,  0.1841]]],\n",
       "\n",
       "\n",
       "         [[[ 0.4356,  0.1950,  0.3224,  ...,  0.4065,  0.3245,  0.3266],\n",
       "           [ 0.4356,  0.3567,  0.6397,  ...,  0.3441,  0.3441,  0.3266],\n",
       "           [ 0.3513,  0.2116,  0.6397,  ...,  0.5399,  0.5399,  0.2292],\n",
       "           ...,\n",
       "           [ 0.3728,  0.5359,  0.5359,  ...,  0.3827,  0.1740,  0.3513],\n",
       "           [ 0.3534,  0.3128,  0.5264,  ...,  0.4766,  0.2997,  0.3513],\n",
       "           [ 0.3128,  0.3128,  0.2494,  ...,  0.5001,  0.5282,  0.5282]]],\n",
       "\n",
       "\n",
       "         [[[-0.1824, -0.0831, -0.0219,  ...,  0.1371, -0.0765,  0.1652],\n",
       "           [-0.1431, -0.1541, -0.0219,  ..., -0.2116, -0.0669,  0.0101],\n",
       "           [-0.1887, -0.2092, -0.0824,  ..., -0.1037,  0.0027,  0.0101],\n",
       "           ...,\n",
       "           [-0.1671, -0.0631, -0.1217,  ..., -0.0307, -0.0573, -0.0573],\n",
       "           [-0.1541, -0.0631,  0.1023,  ..., -0.1074, -0.0678, -0.0678],\n",
       "           [-0.0590, -0.1062,  0.1023,  ..., -0.1709,  0.0051,  0.0051]]]]],\n",
       "       grad_fn=<MaxPool3DWithIndicesBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Cov3DSize(input, cin, cout, kernel_size, stride, padding):\n",
    "    conv = nn.Conv3d(cin, cout, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    output = conv(input)\n",
    "    print(output.shape)\n",
    "    return output\n",
    "\n",
    "def MaxPool3D(input, kernel_size, stride):\n",
    "    pool = nn.MaxPool3d(kernel_size=kernel_size, stride=stride)\n",
    "    output = pool(input)\n",
    "    print(\"MaxPool3D shape: \", output.shape)\n",
    "    return output\n",
    "\n",
    "input = torch.rand(20, 3, 5, 224, 224)\n",
    "input = Cov3DSize(input, 3, 96, kernel_size=(5,7,7), stride=(1,2,2), padding=0)\n",
    "input = MaxPool3D(input, kernel_size=(1,3,3), stride=(1,2,2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxPool2d, MaxPool3d\n",
    "[https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.2346e+00,  3.7939e-01, -1.9367e+00,  1.3094e+00,  4.6663e-01],\n",
      "          [ 3.4819e-01, -5.6895e-01, -1.5108e+00,  7.0780e-01,  9.0777e-01],\n",
      "          [ 4.6561e-01,  2.1563e+00, -2.3255e+00, -6.6472e-01,  5.5147e-01],\n",
      "          [ 9.4951e-01, -3.3740e-01, -6.2161e-02,  4.8511e-01, -1.4107e-01],\n",
      "          [ 6.3537e-01,  1.5464e+00,  2.0419e+00,  2.4523e-01,  2.2240e+00]],\n",
      "\n",
      "         [[-5.4798e-01,  1.6411e-01, -2.4302e-01, -2.1069e+00, -1.1748e+00],\n",
      "          [ 5.8614e-01, -1.0792e+00,  8.6184e-02,  2.6984e-02,  3.7680e-01],\n",
      "          [-4.2325e-02,  5.9603e-01, -5.9892e-01,  6.5933e-01,  9.7178e-01],\n",
      "          [-1.4920e+00, -2.1150e-01, -2.5007e-01, -2.6350e-01, -7.3031e-01],\n",
      "          [-8.8017e-01, -1.0805e+00, -8.5367e-01,  2.3745e+00,  1.7426e+00]],\n",
      "\n",
      "         [[ 3.5111e-01, -1.0492e+00, -6.6652e-01,  1.4015e+00,  2.0608e+00],\n",
      "          [-1.2575e+00, -4.0794e-01,  2.2955e-02, -1.0286e+00,  6.4215e-01],\n",
      "          [-8.6350e-01, -2.7992e-01, -2.8991e-01,  9.1334e-01,  3.0354e-01],\n",
      "          [ 2.1641e-01, -1.9566e-01,  1.0338e+00,  1.1264e+00,  4.0221e-01],\n",
      "          [ 2.0189e+00, -1.6187e+00,  4.1265e-01, -6.0841e-01,  1.8968e-01]],\n",
      "\n",
      "         [[-1.8429e-01,  8.0868e-01,  1.7447e+00, -1.0210e+00,  1.5757e+00],\n",
      "          [-3.1405e-02, -2.0191e+00,  1.1914e+00,  1.1556e+00,  1.6644e+00],\n",
      "          [-1.1875e-01,  9.0442e-01, -1.1304e+00,  6.0358e-01,  1.0516e+00],\n",
      "          [-4.0070e-01, -1.1804e+00,  1.9238e+00, -1.9617e+00,  1.0318e-01],\n",
      "          [ 6.5388e-01,  2.0243e+00, -1.6899e+00, -5.0559e-02,  1.6300e+00]],\n",
      "\n",
      "         [[-9.7397e-01,  1.1398e+00,  1.4172e+00, -2.7411e+00, -8.6943e-01],\n",
      "          [-5.4372e-01,  1.2351e+00,  2.0426e-01,  9.3565e-01,  1.8707e-01],\n",
      "          [-2.0517e+00,  2.0031e+00,  1.2341e+00, -1.8753e-01, -1.8052e-01],\n",
      "          [-1.2378e+00,  1.6180e-01, -7.2887e-01, -1.2642e-01,  1.0855e+00],\n",
      "          [ 2.7149e-01,  1.0281e-01, -6.0565e-01, -8.3267e-02,  1.1175e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5715e-01,  2.4055e+00,  8.4600e-01,  5.2831e-01, -1.1693e+00],\n",
      "          [-5.2568e-01, -1.0548e+00, -6.7944e-01, -8.2646e-01,  2.7494e-01],\n",
      "          [ 6.2002e-01,  1.5460e+00,  1.2003e+00, -3.9887e-01,  1.1501e+00],\n",
      "          [ 6.7257e-02,  1.6964e-01, -6.8771e-01, -1.2509e+00, -5.2786e-03],\n",
      "          [-1.3184e-01,  4.0219e-01, -1.3801e+00, -8.3485e-01,  1.4085e+00]],\n",
      "\n",
      "         [[-9.2888e-01,  3.5968e-01, -1.1723e+00,  2.4273e-01,  1.7805e+00],\n",
      "          [-3.7535e-01, -1.8026e+00,  3.5588e-01,  1.0069e+00, -6.2580e-01],\n",
      "          [-6.8609e-01,  2.6151e-01, -8.5780e-01, -1.0749e-01, -1.1678e-01],\n",
      "          [ 1.0862e-01,  1.2231e+00, -8.5023e-01, -7.5645e-01, -7.0967e-01],\n",
      "          [-2.5306e+00,  9.8693e-02, -1.7506e+00,  1.7387e-01, -3.2363e-01]],\n",
      "\n",
      "         [[-4.2739e-01, -4.9143e-01, -5.2440e-01, -4.3537e-01, -2.7199e+00],\n",
      "          [-8.4410e-01, -4.5774e-01, -5.1657e-01,  7.0558e-01, -1.0244e+00],\n",
      "          [-7.0609e-01,  1.4285e-01,  2.8346e-01, -9.8969e-01,  2.8631e-01],\n",
      "          [ 1.2106e+00, -2.3849e-01,  7.4692e-01,  2.0750e+00,  2.0930e-01],\n",
      "          [ 1.0754e+00,  1.2247e+00,  1.8562e+00, -2.7204e-01, -2.7231e-01]],\n",
      "\n",
      "         [[-4.5841e-02, -2.4852e-01,  8.9912e-02,  3.4969e-01, -2.7855e-01],\n",
      "          [ 4.1676e-02, -1.6586e-01,  4.9496e-01,  1.3699e+00,  6.7802e-01],\n",
      "          [ 5.2222e-01,  1.0178e-01,  1.1983e+00, -4.0592e-01, -9.0113e-01],\n",
      "          [ 1.6810e+00,  1.9428e+00, -9.6141e-01,  1.2544e+00, -8.7635e-01],\n",
      "          [ 2.6562e-01,  1.6067e+00,  1.0692e+00, -4.1490e-01,  8.7475e-01]],\n",
      "\n",
      "         [[-1.3481e+00, -8.2538e-01, -7.3892e-01,  1.0084e+00, -4.9712e-01],\n",
      "          [ 5.7413e-01,  3.6640e-01, -9.4849e-01,  6.9340e-01,  8.8671e-01],\n",
      "          [-1.9473e+00,  1.3217e+00,  4.8450e-01, -1.1724e+00,  2.0151e+00],\n",
      "          [-6.8382e-02, -2.7857e-02, -9.5730e-01, -1.6270e+00, -2.6060e-01],\n",
      "          [-7.3793e-01,  5.3350e-01, -2.9324e-01,  1.0894e+00, -5.4742e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7109e+00,  1.3023e+00, -1.3803e+00, -7.1883e-01,  9.2907e-01],\n",
      "          [ 1.7866e+00, -2.3257e-01,  4.9878e-01, -8.1394e-01, -1.1929e-01],\n",
      "          [ 2.8733e-01, -7.3548e-01, -5.0384e-02, -1.4352e+00, -1.1997e-03],\n",
      "          [ 7.3440e-02,  6.3864e-01,  1.0079e+00,  5.4344e-01,  1.3513e-01],\n",
      "          [-1.4297e+00, -6.2510e-01, -1.5934e+00,  1.1927e+00,  1.6565e+00]],\n",
      "\n",
      "         [[ 1.2001e+00,  9.7253e-02, -1.2458e+00, -5.8182e-01,  1.3888e+00],\n",
      "          [ 1.2897e-01,  5.3670e-01,  5.7344e-01, -9.8593e-02, -9.8506e-01],\n",
      "          [ 7.1238e-01,  7.6798e-01, -9.4886e-01, -1.0887e+00,  7.2283e-01],\n",
      "          [ 8.4153e-01,  1.8844e-02, -1.1192e+00, -3.3701e-01,  3.3998e-01],\n",
      "          [ 3.1752e-01, -1.2712e+00,  1.3944e+00, -9.1838e-01, -1.1232e+00]],\n",
      "\n",
      "         [[-1.1130e+00,  5.8787e-01,  9.0246e-01,  2.3069e+00,  2.4239e-02],\n",
      "          [-2.0671e+00,  5.6272e-01,  9.2889e-01,  1.0599e+00,  6.2464e-01],\n",
      "          [-2.0304e-01, -2.0553e-01, -9.5962e-01,  1.1244e-01, -1.5785e+00],\n",
      "          [-9.3650e-01,  7.8743e-01, -7.1150e-01, -3.3500e-01, -4.7108e-02],\n",
      "          [ 8.3871e-01,  6.4528e-01,  1.0008e+00,  6.7690e-02,  8.1117e-01]],\n",
      "\n",
      "         [[ 6.9302e-01, -1.9515e+00,  2.7061e-01, -6.8106e-01, -2.4271e-01],\n",
      "          [ 1.5973e+00, -2.5634e-01, -2.0160e+00, -1.2460e+00,  7.9957e-01],\n",
      "          [ 9.6181e-01, -9.1129e-01, -1.0838e+00,  1.7153e+00,  1.0231e+00],\n",
      "          [-1.0809e+00, -1.2233e+00, -9.3662e-01,  1.2365e-01, -7.3753e-01],\n",
      "          [-3.2826e-01, -1.0198e+00,  7.8025e-01,  3.0057e-01,  6.3524e-02]],\n",
      "\n",
      "         [[ 7.3830e-01, -3.1824e-01,  7.4991e-01,  1.7783e+00,  1.3528e+00],\n",
      "          [-1.4204e-01, -1.1918e+00,  1.1593e+00,  7.9833e-03,  6.1654e-01],\n",
      "          [-4.8035e-01,  3.1101e-01,  7.3892e-02,  6.1681e-01,  1.8110e-01],\n",
      "          [-7.3294e-01,  2.6680e-01, -1.0728e+00,  2.0565e+00,  5.6888e-01],\n",
      "          [-4.4930e-01,  4.1409e-01, -1.0088e+00,  2.5203e-02, -1.3462e-01]]]])\n",
      "torch.Size([5, 5, 5])\n",
      "torch.Size([5, 5, 5])\n",
      "torch.Size([5, 5, 5])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaptiveAvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghm/miniconda3/envs/py38-torch100-cu11/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "adp_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
    "input = torch.randn(1, 3, 512, 512)\n",
    "output = adp_pool(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "The total number of parameters in module is:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "print([i for i in upsample.parameters()])\n",
    "\n",
    "print(\"The total number of parameters in module is:\")\n",
    "pytorch_total_params = sum(p.numel() for p in upsample.parameters())\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2, 2]) torch.Size([1, 2, 2, 2])\n",
      "tensor([[[[ 0.3909, -0.1451],\n",
      "          [-1.6407, -1.0667]],\n",
      "\n",
      "         [[-0.2172,  0.2076],\n",
      "          [ 1.9867,  0.7596]],\n",
      "\n",
      "         [[ 1.0905, -0.9003],\n",
      "          [ 0.4875,  0.6588]],\n",
      "\n",
      "         [[ 1.1397,  1.1139],\n",
      "          [ 0.7776,  1.0089]]]])\n",
      "tensor([[[[ 0.3909, -0.1451],\n",
      "          [-1.6407, -1.0667]],\n",
      "\n",
      "         [[-0.2172,  0.2076],\n",
      "          [ 1.9867,  0.7596]]]])\n",
      "tensor([[[[ 1.0905, -0.9003],\n",
      "          [ 0.4875,  0.6588]],\n",
      "\n",
      "         [[ 1.1397,  1.1139],\n",
      "          [ 0.7776,  1.0089]]]])\n"
     ]
    }
   ],
   "source": [
    "style = torch.randn(1, 4, 2, 2)\n",
    "gamma, beta = style.chunk(2, 1)\n",
    "print(gamma.shape, beta.shape)\n",
    "print(style)\n",
    "print(gamma)\n",
    "print(beta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Upsample class\n",
    "https://www.cnblogs.com/wanghui-garcia/p/11399053.html\n",
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-upsample-for-upsampling-with-pytorch.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 3)\n",
      "torch.Size([1, 3, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghm/miniconda3/envs/py38-torch100-cu11/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "input_data = np.loadtxt(\"./data/02933112.23637357c4e5efb653c80c0a3ef7382.xyz\").astype(np.float32)\n",
    "print(input_data.shape)\n",
    "input_data = torch.from_numpy(input_data).unsqueeze(0)\n",
    "input_data = input_data.permute(0, 2, 1)\n",
    "upsample_module = nn.Upsample(scale_factor=4, mode=\"linear\")\n",
    "output = upsample_module(input_data)\n",
    "print(output.shape)\n",
    "output_numpy = output.squeeze().permute(1, 0).numpy()\n",
    "np.savetxt(\"./data/upsampled_points_X4.xyz\", output_numpy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7364, -0.5942, -0.3573],\n",
      "        [ 0.6877,  0.4940,  1.5346]])\n",
      "tensor([[0.2767, 0.3190, 0.4043],\n",
      "        [0.2406, 0.1982, 0.5612]])\n",
      "tensor([[0.2767, 0.3190, 0.4043],\n",
      "        [0.2406, 0.1982, 0.5612]])\n",
      "tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)\n",
    "\n",
    "output = input.softmax(dim=1)\n",
    "print(output)\n",
    "\n",
    "output = output.sum(dim=1)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38-torch100-cu11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56d0891f27fe2db3830dc2a05bc70f05135b0c30ed7c190a150d1aca2da3af60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
